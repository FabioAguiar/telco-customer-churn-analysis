{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8616a8",
   "metadata": {},
   "source": [
    "\n",
    "# N2 ‚Äî Modelagem (Painel de Controle) ‚Äî **Patch de caminhos aplicado**\n",
    "\n",
    "- Corrigido: cria√ß√£o de `reports/` e `artifacts/` no diret√≥rio dos notebooks.  \n",
    "- Agora o N2 **descobre a raiz do projeto** pelo `config/defaults.json` e usa essa raiz para todos os caminhos.  \n",
    "- `data_processed_file` √© uma **chave de config** (n√£o √© fun√ß√£o). Se voc√™ definir o nome do arquivo l√°, o N2 usa direto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de01a6f-04c5-4887-ba8c-a31ab8065164",
   "metadata": {},
   "source": [
    "# üìò Bootstrap do N2 ‚Äî Raiz do Projeto, Configura√ß√£o e Dataset\n",
    "\n",
    "Nesta etapa inicial, o notebook realiza o **bootstrap** do ambiente de modelagem, preparando todo o contexto necess√°rio para o treinamento de modelos supervisionados.\n",
    "\n",
    "## Principais a√ß√µes executadas\n",
    "\n",
    "1. **Localiza√ß√£o autom√°tica da raiz do projeto (`PROJECT_ROOT`)**\n",
    "   O c√≥digo sobe a √°rvore de diret√≥rios at√© encontrar o arquivo `config/defaults.json`.\n",
    "   Esse processo garante que o notebook funcione corretamente mesmo se for aberto a partir de subpastas (como `notebooks/`).\n",
    "\n",
    "2. **Inje√ß√£o do caminho da raiz e da pasta `utils/` no `sys.path`**\n",
    "   Essa etapa permite que os m√≥dulos auxiliares do projeto (`utils/utils_data.py`) sejam importados normalmente, sem precisar ajustar manualmente os caminhos no ambiente.\n",
    "\n",
    "3. **Carregamento da configura√ß√£o global (`defaults.json`)**\n",
    "   Todas as defini√ß√µes de comportamento ‚Äî como colunas-alvo, escalonamento, propor√ß√£o de teste, regras de outliers e encoding ‚Äî s√£o lidas diretamente desse arquivo centralizado, assegurando consist√™ncia entre N1, N2 e N3.\n",
    "\n",
    "4. **Garantia dos diret√≥rios padr√£o**\n",
    "   As pastas `artifacts/`, `reports/` e `artifacts/models/` s√£o criadas automaticamente, caso n√£o existam, organizando as sa√≠das do pipeline de modelagem.\n",
    "\n",
    "5. **Descoberta do dataset processado**\n",
    "   O notebook identifica e carrega o arquivo final preparado no N1, normalmente localizado em `data/processed/processed.parquet`.\n",
    "   Caso n√£o o encontre, uma mensagem orienta o usu√°rio a revisar o `defaults.json` ou reexecutar o N1.\n",
    "\n",
    "6. **Leitura do dataset e defini√ß√£o da vari√°vel alvo (`TARGET_COL`)**\n",
    "   O dataset √© carregado conforme o formato detectado (Parquet, CSV ou Excel).\n",
    "   Em seguida, a vari√°vel alvo √© localizada automaticamente com base na configura√ß√£o, e as vari√°veis independentes (`X`) s√£o separadas.\n",
    "\n",
    "7. **Resumo r√°pido do ambiente e do dataset**\n",
    "   Ao final, s√£o exibidas informa√ß√µes gerais sobre o projeto, o arquivo carregado, dimens√µes, mem√≥ria, contagem de colunas por tipo, nulos e distribui√ß√£o da vari√°vel alvo ‚Äî criando um panorama r√°pido e audit√°vel do ponto de partida para a modelagem.\n",
    "\n",
    "---\n",
    "\n",
    "> **Em resumo:**  \n",
    "> Esta c√©lula garante que o N2 sempre inicie em um ambiente limpo, conectado √† raiz do projeto, com acesso √†s utilidades centralizadas, configura√ß√£o carregada e dataset processado prontos para o treinamento dos modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9efae474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PROJECT_ROOT: C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\n",
      "[INFO] sys.path inclui raiz? True\n",
      "[INFO] sys.path inclui utils? True\n",
      "[INFO] PROJECT_ROOT: C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\n",
      "[INFO] PROJECT_ROOT: C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\n",
      "[INFO] [ensure_dirs] artifacts=C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\\artifacts | reports=C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\\reports | models=C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\\artifacts\\models\n",
      "[INFO] PROJECT_ROOT: C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\n",
      "[INFO] Active config: {\n",
      "  \"infer_types\": true,\n",
      "  \"cast_numeric_like\": true,\n",
      "  \"strip_whitespace\": true,\n",
      "  \"handle_missing\": true,\n",
      "  \"missing_strategy\": \"simple\",\n",
      "  \"detect_outliers\": true,\n",
      "  \"outlier_method\": \"iqr\",\n",
      "  \"outliers\": {\n",
      "    \"cols\": null,\n",
      "    \"exclude_cols\": [\n",
      "      \"customerID\"\n",
      "    ],\n",
      "    \"exclude_binaries\": true,\n",
      "    \"iqr_factor\": 1.5,\n",
      "    \"z_threshold\": 3.0,\n",
      "    \"persist_summary\": true,\n",
      "    \"persist_relpath\": \"outliers/summary.csv\"\n",
      "  },\n",
      "  \"deduplicate\": {\n",
      "    \"subset\": null,\n",
      "    \"keep\": \"first\",\n",
      "    \"log_enabled\": true,\n",
      "    \"log_relpath\": \"duplicates.csv\"\n",
      "  },\n",
      "  \"deduplicate_subset\": null,\n",
      "  \"deduplicate_keep\": \"first\",\n",
      "  \"deduplicate_log\": true,\n",
      "  \"deduplicate_log_filename\": \"duplicates.csv\",\n",
      "  \"encode_categoricals\": true,\n",
      "  \"encoding_type\": \"onehot\",\n",
      "  \"scale_numeric\": true,\n",
      "  \"scaler\": \"minmax\",\n",
      "  \"date_features\": true,\n",
      "  \"text_features\": true,\n",
      "  \"feature_engineering\": {\n",
      "    \"enable_default_rules\": true,\n",
      "    \"log1p_cols\": [],\n",
      "    \"ratios\": [],\n",
      "    \"binaries\": [],\n",
      "    \"date_parts\": []\n",
      "  },\n",
      "  \"export_interim\": true,\n",
      "  \"normalize_categories\": true,\n",
      "  \"export_processed\": true,\n",
      "  \"reporting\": {\n",
      "    \"manifest_enabled\": true\n",
      "  },\n",
      "  \"target\": {\n",
      "    \"name\": \"Churn\",\n",
      "    \"source\": \"Churn\",\n",
      "    \"positive\": \"Yes\",\n",
      "    \"negative\": \"No\"\n",
      "  },\n",
      "  \"dates\": {\n",
      "    \"detect_regex\": \"(date|data|dt_|_dt$|_date$|_at$|time|timestamp|created|updated)\",\n",
      "    \"explicit_cols\": [],\n",
      "    \"dayfirst\": false,\n",
      "    \"utc\": false,\n",
      "    \"formats\": [],\n",
      "    \"min_ratio\": 0.8,\n",
      "    \"report_path\": \"date_parse_report.csv\"\n",
      "  },\n",
      "  \"artifacts_dir\": \"artifacts\",\n",
      "  \"data_raw_dir\": \"data/raw\",\n",
      "  \"data_processed_dir\": \"data/processed\",\n",
      "  \"data_processed_file\": \"processed.parquet\",\n",
      "  \"target_column\": \"Churn\",\n",
      "  \"test_size\": 0.2,\n",
      "  \"random_state\": 42,\n",
      "  \"class_map\": {\n",
      "    \"Yes\": 1,\n",
      "    \"No\": 0\n",
      "  }\n",
      "}\n",
      "[INFO] Processed file: C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\\data\\processed\\processed.parquet\n",
      "[INFO] TARGET_COL = Churn\n",
      "(7043, 127) (7043,)\n",
      "\n",
      "==============================================================================\n",
      "N2 ‚Äî Resumo do bootstrap e leitura do dataset\n",
      "==============================================================================\n",
      "‚Ä¢ Projeto (PROJECT_ROOT): C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\n",
      "‚Ä¢ Arquivo lido         : C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\\data\\processed\\processed.parquet\n",
      "‚Ä¢ Formato              : .parquet\n",
      "‚Ä¢ Dimens√£o             : 7043 linhas √ó 128 colunas\n",
      "‚Ä¢ Mem√≥ria estimada     : 9.39 MB\n",
      "‚Ä¢ Colunas por tipo     : num=62 | cat=66 | other=0\n",
      "‚Ä¢ Nulos (total)        : 11 c√©lulas nulas em 1 colunas com nulos\n",
      "‚Ä¢ Target               : 'Churn' (valores √∫nicos=2)\n",
      "‚Ä¢ Distribui√ß√£o da target:\n",
      "       count    pct\n",
      "Churn              \n",
      "no      5174  73.46\n",
      "yes     1869  26.54\n",
      "------------------------------------------------------------------------------\n",
      "Par√¢metros previstos (antes do split):\n",
      "‚Ä¢ test_size   = 0.2\n",
      "‚Ä¢ random_state= 42\n",
      "‚Ä¢ scale_numeric (pr√©-processamento) = True\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    RocCurveDisplay,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import ipywidgets as W\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ------------------------------\n",
    "# Bootstrap de caminho: encontra raiz e injeta no sys.path\n",
    "# ------------------------------\n",
    "def _find_up(relative_path: str, start: Path | None = None) -> Path | None:\n",
    "    start = start or Path.cwd()\n",
    "    rel = Path(relative_path)\n",
    "    for base in (start, *start.parents):\n",
    "        cand = base / rel\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "_cfg = _find_up(\"config/defaults.json\")\n",
    "if _cfg is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"config/defaults.json n√£o encontrado. Abra o notebook N2 dentro da estrutura do projeto.\"\n",
    "    )\n",
    "\n",
    "PROJECT_ROOT = _cfg.parent.parent  # .../config/defaults.json -> raiz\n",
    "# injeta raiz e pasta utils no sys.path (para permitir `from utils...`)\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "utils_dir = PROJECT_ROOT / \"utils\"\n",
    "if utils_dir.exists() and str(utils_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(utils_dir))\n",
    "\n",
    "print(\"[INFO] PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"[INFO] sys.path inclui raiz?\", str(PROJECT_ROOT) in sys.path)\n",
    "print(\"[INFO] sys.path inclui utils?\", str(utils_dir) in sys.path)\n",
    "\n",
    "# >>>>> utilidades agora v√™m do utils_data <<<<<\n",
    "from utils.utils_data import (\n",
    "    get_project_root, load_config, ensure_dirs, discover_processed_path,\n",
    "    summarize_columns, compute_metrics, try_plot_roc, persist_artifacts\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Carrega config, garante dirs e resolve dataset processado\n",
    "# ------------------------------\n",
    "cfg = load_config()\n",
    "artifacts_dir, reports_dir, models_dir = ensure_dirs(cfg)\n",
    "processed_path = discover_processed_path(cfg)\n",
    "print(\"[INFO] Active config:\", json.dumps(cfg, indent=2, ensure_ascii=False))\n",
    "print(\"[INFO] Processed file:\", processed_path)\n",
    "\n",
    "# Leitura do dataset (suporte a parquet/csv/xlsx)\n",
    "if processed_path.suffix.lower() in [\".parquet\", \".pq\"]:\n",
    "    df = pd.read_parquet(processed_path)\n",
    "elif processed_path.suffix.lower() == \".csv\":\n",
    "    df = pd.read_csv(processed_path)\n",
    "elif processed_path.suffix.lower() in [\".xlsx\", \".xls\"]:\n",
    "    df = pd.read_excel(processed_path)\n",
    "else:\n",
    "    raise ValueError(f\"Extens√£o n√£o suportada: {processed_path.suffix}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Fun√ß√£o de pr√©-processamento ‚Äî **fica exposta no notebook**\n",
    "# ------------------------------\n",
    "def build_preprocess(numeric_cols, categorical_cols, scale_numeric=True):\n",
    "    num_steps = [(\"imputer\", SimpleImputer(strategy=\"mean\"))]\n",
    "    if scale_numeric:\n",
    "        num_steps.append((\"scaler\", StandardScaler()))\n",
    "    numeric_transformer = Pipeline(steps=num_steps)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "    ])\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# Confirmar leitura e target\n",
    "df.head(3)\n",
    "\n",
    "TARGET_COL = df.columns[df.columns.str.lower() == str(cfg.get(\"target_column\", \"target\")).lower()]\n",
    "TARGET_COL = TARGET_COL[0] if len(TARGET_COL) else cfg.get(\"target_column\", \"target\")\n",
    "print(\"[INFO] TARGET_COL =\", TARGET_COL)\n",
    "assert TARGET_COL in df.columns, f\"Target '{TARGET_COL}' n√£o encontrada no dataset.\"\n",
    "\n",
    "# Separar X/y\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# === Painel r√°pido de status do N2 (resumo explicativo) ===\n",
    "from pathlib import Path\n",
    "\n",
    "def _fmt_mb(n_bytes: int) -> str:\n",
    "    return f\"{n_bytes / (1024**2):.2f} MB\"\n",
    "\n",
    "def _pct(n, d) -> str:\n",
    "    return f\"{(100*n/d):.2f}%\" if d else \"n/a\"\n",
    "\n",
    "# 1) Info do dataset\n",
    "n_rows, n_cols = df.shape\n",
    "mem_bytes = df.memory_usage(deep=True).sum()\n",
    "\n",
    "# 2) Tipos de colunas\n",
    "num_cols, cat_cols, other_cols = summarize_columns(df)\n",
    "\n",
    "# 3) Nulos (vis√£o geral)\n",
    "null_total = int(df.isna().sum().sum())\n",
    "null_any_cols = int((df.isna().sum() > 0).sum())\n",
    "\n",
    "# 4) Target\n",
    "target_name = TARGET_COL\n",
    "n_unique_target = int(pd.Series(y).nunique())\n",
    "target_counts = y.value_counts(dropna=False)\n",
    "target_pct = (target_counts / len(y) * 100).round(2)\n",
    "\n",
    "# 5) Split config (sem executar o split ainda; apenas ecoa a config)\n",
    "test_size = cfg.get(\"test_size\", 0.2)\n",
    "random_state = cfg.get(\"random_state\", 42)\n",
    "scale_numeric = bool(cfg.get(\"scale_numeric\", True))\n",
    "\n",
    "print(\"\\n\" + \"=\"*78)\n",
    "print(\"N2 ‚Äî Resumo do bootstrap e leitura do dataset\")\n",
    "print(\"=\"*78)\n",
    "print(f\"‚Ä¢ Projeto (PROJECT_ROOT): {PROJECT_ROOT}\")\n",
    "print(f\"‚Ä¢ Arquivo lido         : {processed_path}\")\n",
    "print(f\"‚Ä¢ Formato              : {processed_path.suffix.lower()}\")\n",
    "print(f\"‚Ä¢ Dimens√£o             : {n_rows} linhas √ó {n_cols} colunas\")\n",
    "print(f\"‚Ä¢ Mem√≥ria estimada     : {_fmt_mb(mem_bytes)}\")\n",
    "print(f\"‚Ä¢ Colunas por tipo     : num={len(num_cols)} | cat={len(cat_cols)} | other={len(other_cols)}\")\n",
    "print(f\"‚Ä¢ Nulos (total)        : {null_total} c√©lulas nulas em {null_any_cols} colunas com nulos\")\n",
    "print(f\"‚Ä¢ Target               : '{target_name}' (valores √∫nicos={n_unique_target})\")\n",
    "\n",
    "# Distribui√ß√£o da target (top 5 caso multiclasses extensas)\n",
    "print(\"‚Ä¢ Distribui√ß√£o da target:\")\n",
    "if len(target_counts) > 5:\n",
    "    to_show = target_counts.head(5)\n",
    "    to_show_pct = target_pct.head(5)\n",
    "    others = len(target_counts) - 5\n",
    "    print((pd.DataFrame({'count': to_show, 'pct': to_show_pct}).to_string()))\n",
    "    print(f\"  ... (+{others} classes)\")\n",
    "else:\n",
    "    print((pd.DataFrame({'count': target_counts, 'pct': target_pct}).to_string()))\n",
    "\n",
    "print(\"-\"*78)\n",
    "print(\"Par√¢metros previstos (antes do split):\")\n",
    "print(f\"‚Ä¢ test_size   = {test_size}\")\n",
    "print(f\"‚Ä¢ random_state= {random_state}\")\n",
    "print(f\"‚Ä¢ scale_numeric (pr√©-processamento) = {scale_numeric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99686093",
   "metadata": {},
   "source": [
    "# üîÑ Carregamento da Config e Descoberta do Dataset Processado\n",
    "\n",
    "Nesta etapa, o N2 sincroniza o ambiente de modelagem com as **defini√ß√µes globais** do projeto e obt√©m o **dataset final** preparado no N1.\n",
    "\n",
    "## O que acontece aqui\n",
    "\n",
    "1. **Leitura da configura√ß√£o (`defaults.json`)**\n",
    "   - Centraliza par√¢metros reutiliz√°veis entre N1, N2 e N3 (ex.: `target_column`, `test_size`, `random_state`, `scale_numeric`, regras de *feature engineering*).\n",
    "   - Garante consist√™ncia entre projetos e evita \"configura√ß√£o espalhada\" dentro dos notebooks.\n",
    "\n",
    "2. **Garantia de diret√≥rios de sa√≠da**\n",
    "   - Cria (se necess√°rio) `artifacts/`, `reports/` e `artifacts/models/` na **raiz do projeto**.\n",
    "   - Mant√©m modelos, m√©tricas e manifestos organizados em uma estrutura previs√≠vel.\n",
    "\n",
    "3. **Descoberta do dataset processado**\n",
    "   - Resolve automaticamente o arquivo de `data/processed` (preferindo `data_processed_file`; caso ausente, tenta por extens√£o).\n",
    "   - Erros informativos orientam a ajustar o `defaults.json` ou reexecutar o N1, caso o arquivo n√£o exista.\n",
    "\n",
    "4. **Leitura robusta do dataset**\n",
    "   - Suporte a Parquet/CSV/Excel, com *fallback* de engine para Parquet quando necess√°rio.\n",
    "   - Erros de leitura s√£o encapsulados com mensagens claras, √∫teis para depura√ß√£o.\n",
    "\n",
    "5. **Determina√ß√£o da vari√°vel alvo (`TARGET_COL`)**\n",
    "   - Identifica a coluna alvo via `target_column` (ou `target.name`) de forma *case-insensitive*.\n",
    "   - Se a coluna n√£o existir, interrompe com instru√ß√£o objetiva de corre√ß√£o.\n",
    "\n",
    "6. **Separa√ß√£o de vari√°veis (X/y) e diagn√≥stico da target**\n",
    "   - Remove a coluna alvo de `X` e mant√©m `y` com a distribui√ß√£o original.\n",
    "   - Exibe **n√∫mero de nulos**, **n¬∫ de classes** e **distribui√ß√£o** (contagem e %).\n",
    "   - Caso exista `class_map`, informa a **cobertura potencial** do mapeamento (sem alterar `y`).\n",
    "\n",
    "## Por que esta etapa √© importante?\n",
    "\n",
    "- Garante que o N2 est√° alinhado com a **configura√ß√£o oficial** do projeto.\n",
    "- Confere se o dataset final do N1 est√° **presente e √≠ntegro** antes de prosseguir.\n",
    "- Fornece um **resumo audit√°vel** (formas de `X/y`, distribui√ß√£o da target), √∫til para apresenta√ß√µes e *debug*.\n",
    "\n",
    "---\n",
    "\n",
    "> **Dica:** Se quiser normalizar a classe alvo (por exemplo, trocando *Yes/No* por 1/0), fa√ßa uma c√≥pia (`y_mapped`) usando `class_map` **apenas** depois de validar a distribui√ß√£o original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139d46e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PROJECT_ROOT: C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\n",
      "[INFO] PROJECT_ROOT: C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\n",
      "[INFO] [ensure_dirs] artifacts=C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\\artifacts | reports=C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\\reports | models=C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\\artifacts\\models\n",
      "[INFO] PROJECT_ROOT: C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\n",
      "[INFO] Active config: {\n",
      "  \"infer_types\": true,\n",
      "  \"cast_numeric_like\": true,\n",
      "  \"strip_whitespace\": true,\n",
      "  \"handle_missing\": true,\n",
      "  \"missing_strategy\": \"simple\",\n",
      "  \"detect_outliers\": true,\n",
      "  \"outlier_method\": \"iqr\",\n",
      "  \"outliers\": {\n",
      "    \"cols\": null,\n",
      "    \"exclude_cols\": [\n",
      "      \"customerID\"\n",
      "    ],\n",
      "    \"exclude_binaries\": true,\n",
      "    \"iqr_factor\": 1.5,\n",
      "    \"z_threshold\": 3.0,\n",
      "    \"persist_summary\": true,\n",
      "    \"persist_relpath\": \"outliers/summary.csv\"\n",
      "  },\n",
      "  \"deduplicate\": {\n",
      "    \"subset\": null,\n",
      "    \"keep\": \"first\",\n",
      "    \"log_enabled\": true,\n",
      "    \"log_relpath\": \"duplicates.csv\"\n",
      "  },\n",
      "  \"deduplicate_subset\": null,\n",
      "  \"deduplicate_keep\": \"first\",\n",
      "  \"deduplicate_log\": true,\n",
      "  \"deduplicate_log_filename\": \"duplicates.csv\",\n",
      "  \"encode_categoricals\": true,\n",
      "  \"encoding_type\": \"onehot\",\n",
      "  \"scale_numeric\": true,\n",
      "  \"scaler\": \"minmax\",\n",
      "  \"date_features\": true,\n",
      "  \"text_features\": true,\n",
      "  \"feature_engineering\": {\n",
      "    \"enable_default_rules\": true,\n",
      "    \"log1p_cols\": [],\n",
      "    \"ratios\": [],\n",
      "    \"binaries\": [],\n",
      "    \"date_parts\": []\n",
      "  },\n",
      "  \"export_interim\": true,\n",
      "  \"normalize_categories\": true,\n",
      "  \"export_processed\": true,\n",
      "  \"reporting\": {\n",
      "    \"manifest_enabled\": true\n",
      "  },\n",
      "  \"target\": {\n",
      "    \"name\": \"Churn\",\n",
      "    \"source\": \"Churn\",\n",
      "    \"positive\": \"Yes\",\n",
      "    \"negative\": \"No\"\n",
      "  },\n",
      "  \"dates\": {\n",
      "    \"detect_regex\": \"(date|data|dt_|_dt$|_date$|_at$|time|timestamp|created|updated)\",\n",
      "    \"explicit_cols\": [],\n",
      "    \"dayfirst\": false,\n",
      "    \"utc\": false,\n",
      "    \"formats\": [],\n",
      "    \"min_ratio\": 0.8,\n",
      "    \"report_path\": \"date_parse_report.csv\"\n",
      "  },\n",
      "  \"artifacts_dir\": \"artifacts\",\n",
      "  \"data_raw_dir\": \"data/raw\",\n",
      "  \"data_processed_dir\": \"data/processed\",\n",
      "  \"data_processed_file\": \"processed.parquet\",\n",
      "  \"target_column\": \"Churn\",\n",
      "  \"test_size\": 0.2,\n",
      "  \"random_state\": 42,\n",
      "  \"class_map\": {\n",
      "    \"Yes\": 1,\n",
      "    \"No\": 0\n",
      "  }\n",
      "}\n",
      "[INFO] Processed file: C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template\\data\\processed\\processed.parquet\n",
      "[INFO] Target: Churn\n",
      "[CHECK] Target nulos=0 | classes √∫nicas=2\n",
      "[CHECK] Distribui√ß√£o da target (top 5):\n",
      "       count    pct\n",
      "Churn              \n",
      "no      5174  73.46\n",
      "yes     1869  26.54\n",
      "[INFO] Shapes -> X: (7043, 127) | y: (7043,)\n",
      "[INFO] class_map detectado. Mapeamento poss√≠vel em 100.0% das linhas.\n"
     ]
    }
   ],
   "source": [
    "# === Carregamento da config e descoberta do dataset processado (robusto) ===\n",
    "\n",
    "cfg = load_config()\n",
    "artifacts_dir, reports_dir, models_dir = ensure_dirs(cfg)\n",
    "\n",
    "processed_path = discover_processed_path(cfg)\n",
    "print(\"[INFO] Active config:\", json.dumps(cfg, indent=2, ensure_ascii=False))\n",
    "print(\"[INFO] Processed file:\", processed_path)\n",
    "\n",
    "# Leitura do dataset (robusta e informativa)\n",
    "suffix = processed_path.suffix.lower()\n",
    "try:\n",
    "    if suffix in (\".parquet\", \".pq\"):\n",
    "        try:\n",
    "            df = pd.read_parquet(processed_path, engine=\"pyarrow\")\n",
    "        except Exception:\n",
    "            # fallback √∫til quando pyarrow n√£o est√° dispon√≠vel\n",
    "            df = pd.read_parquet(processed_path, engine=\"fastparquet\")\n",
    "    elif suffix == \".csv\":\n",
    "        # low_memory=False evita dtypes quebrados; encoding utf-8 por padr√£o\n",
    "        df = pd.read_csv(processed_path, low_memory=False, encoding=\"utf-8\")\n",
    "    elif suffix in (\".xlsx\", \".xls\"):\n",
    "        df = pd.read_excel(processed_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Extens√£o n√£o suportada: {suffix}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Falha ao ler '{processed_path.name}': {type(e).__name__}: {e}\")\n",
    "\n",
    "# Determina√ß√£o tolerante da coluna alvo\n",
    "cfg_target = (\n",
    "    (cfg.get(\"target_column\"))\n",
    "    or (cfg.get(\"target\", {}) or {}).get(\"name\")\n",
    "    or \"target\"\n",
    ")\n",
    "cands = [c for c in df.columns if c.lower() == str(cfg_target).lower()]\n",
    "if not cands:\n",
    "    raise KeyError(\n",
    "        f\"Target '{cfg_target}' n√£o encontrada no dataset. \"\n",
    "        f\"Defina corretamente 'target_column' (ou target.name) em config/defaults.json.\"\n",
    "    )\n",
    "TARGET_COL = cands[0]\n",
    "print(f\"[INFO] Target: {TARGET_COL}\")\n",
    "\n",
    "# Separar X/y\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# Diagn√≥stico r√°pido da target\n",
    "n_null = int(pd.isna(y).sum())\n",
    "n_classes = int(pd.Series(y).nunique(dropna=True))\n",
    "print(f\"[CHECK] Target nulos={n_null} | classes √∫nicas={n_classes}\")\n",
    "print(\"[CHECK] Distribui√ß√£o da target (top 5):\")\n",
    "print((y.value_counts(dropna=False).head(5).to_frame(\"count\")\n",
    "         .assign(pct=lambda d: (d[\"count\"]/len(y)*100).round(2))))\n",
    "\n",
    "# (Opcional) Pr√©-visualiza√ß√£o do shape de X/y\n",
    "print(f\"[INFO] Shapes -> X: {X.shape} | y: {y.shape}\")\n",
    "\n",
    "# (Opcional) Classe-alvo mapeada (somente mostra, n√£o sobrescreve)\n",
    "class_map = cfg.get(\"class_map\") or {}\n",
    "if class_map:\n",
    "    # mapeamento case-insensitive para exibir se seria poss√≠vel mapear\n",
    "    inv = {str(k).strip().lower(): v for k, v in class_map.items()}\n",
    "    preview = (y.astype(str).str.strip().str.lower()).map(inv)\n",
    "    ok_ratio = preview.notna().mean()\n",
    "    print(f\"[INFO] class_map detectado. Mapeamento poss√≠vel em {ok_ratio:.1%} das linhas.\")\n",
    "    if ok_ratio < 1.0:\n",
    "        print(\"[AVISO] H√° valores em y que n√£o casam com class_map; revise 'class_map' no defaults.json.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81e0a4",
   "metadata": {},
   "source": [
    "# ‚úÇÔ∏è Split Treino/Teste e Resumo de Colunas\n",
    "\n",
    "Nesta etapa, preparamos os dados para a modelagem supervisionada, separando vari√°veis independentes (`X`) da vari√°vel alvo (`y`) e realizando o **split treino/teste** com par√¢metros consistentes definidos no `defaults.json`.\n",
    "\n",
    "## O que acontece aqui\n",
    "\n",
    "1. **Separa√ß√£o de X e y**\n",
    "   - Remove-se a coluna alvo (`TARGET_COL`) de `X` e mant√©m-se `y` intacta para preservar a distribui√ß√£o original.\n",
    "\n",
    "2. **Resumo de tipos de colunas**\n",
    "   - Gera contagens de colunas **num√©ricas**, **categ√≥ricas** e **outras** (se houver).\n",
    "   - Este diagn√≥stico orienta a constru√ß√£o do `ColumnTransformer` na pr√≥xima etapa.\n",
    "\n",
    "3. **Split treino/teste com estratifica√ß√£o**\n",
    "   - Usa `test_size` e `random_state` da configura√ß√£o.\n",
    "   - Ativa `stratify=y` sempre que houver mais de uma classe, mantendo a **mesma propor√ß√£o** de classes em `train` e `test`.\n",
    "\n",
    "4. **Diagn√≥sticos √∫teis**\n",
    "   - Compara a **distribui√ß√£o da target** no conjunto geral vs. `train` vs. `test` (contagem e %).\n",
    "   - Sinaliza **desbalanceamento** quando a classe majorit√°ria ultrapassa um limite (ex.: 80%).\n",
    "   - Detecta **categorias raras** (freq. < 5 no `train`) em vari√°veis categ√≥ricas ‚Äî √∫til para evitar explos√£o do One-Hot e para tratar identificadores.\n",
    "\n",
    "## Observa√ß√µes frequentes\n",
    "\n",
    "- **IDs como `customerID`** aparecem como in√∫meras categorias raras. Em geral, **n√£o devem ser usadas como preditores**, pois s√£o identificadores sem rela√ß√£o causal. Recomenda-se **remov√™-las** de `X` (ou exclu√≠-las da lista de categ√≥ricas antes do One-Hot).\n",
    "- Se houver **alto cardinalidade** e a vari√°vel for preditiva, considere t√©cnicas espec√≠ficas (target encoding, hashing, catboost encoder). Neste template, mantemos a abordagem pedag√≥gica e transparente com One-Hot e remo√ß√£o de IDs.\n",
    "\n",
    "## Pr√≥ximo passo\n",
    "\n",
    "- Construir o **pr√©-processamento** com `ColumnTransformer` (imputa√ß√£o + One-Hot para categ√≥ricas + escala opcional para num√©ricas) e ajustar no conjunto de treino.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8845ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Colunas num√©ricas: 62 | categ√≥ricas: 65 | ignoradas: 0\n",
      "[INFO] Split params -> test_size=0.2 | random_state=42 | stratify=True\n",
      "[INFO] X_train: (5634, 127) | X_test: (1409, 127)\n",
      "\n",
      "[CHECK] Distribui√ß√£o da target ‚Äî geral / train / test\n",
      "‚Ä¢ Geral:\n",
      "        count    pct\n",
      "Churn              \n",
      "no      5174  73.46\n",
      "yes     1869  26.54\n",
      "‚Ä¢ Train:\n",
      "        count    pct\n",
      "Churn              \n",
      "no      4139  73.46\n",
      "yes     1495  26.54\n",
      "‚Ä¢ Test:\n",
      "        count    pct\n",
      "Churn              \n",
      "no      1035  73.46\n",
      "yes      374  26.54\n",
      "[INFO] Colunas categ√≥ricas com categorias raras (<5 amostras) no train:\n",
      " - customerID: 5634 categorias raras\n"
     ]
    }
   ],
   "source": [
    "# === Split treino/teste e resumo de colunas (diagn√≥stico refor√ßado) ===\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# Resumo de colunas do X\n",
    "num_cols, cat_cols, other_cols = summarize_columns(X)\n",
    "print(f\"[INFO] Colunas num√©ricas: {len(num_cols)} | categ√≥ricas: {len(cat_cols)} | ignoradas: {len(other_cols)}\")\n",
    "\n",
    "# Par√¢metros do split (eco expl√≠cito)\n",
    "test_size = float(cfg.get(\"test_size\", 0.2))\n",
    "random_state = int(cfg.get(\"random_state\", 42))\n",
    "do_stratify = (pd.Series(y).nunique() > 1)\n",
    "print(f\"[INFO] Split params -> test_size={test_size} | random_state={random_state} | stratify={do_stratify}\")\n",
    "\n",
    "# Split com estratifica√ß√£o quando aplic√°vel\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    stratify=y if do_stratify else None,\n",
    ")\n",
    "print(f\"[INFO] X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
    "\n",
    "# Confer√™ncia da propor√ß√£o da target no geral vs train/test\n",
    "def _dist(s):\n",
    "    vc = s.value_counts(dropna=False)\n",
    "    pct = (vc / len(s) * 100).round(2)\n",
    "    return pd.DataFrame({\"count\": vc, \"pct\": pct})\n",
    "\n",
    "print(\"\\n[CHECK] Distribui√ß√£o da target ‚Äî geral / train / test\")\n",
    "disp_overall = _dist(y)\n",
    "disp_train   = _dist(y_train)\n",
    "disp_test    = _dist(y_test)\n",
    "print(\"‚Ä¢ Geral:\\n\", disp_overall.to_string())\n",
    "print(\"‚Ä¢ Train:\\n\", disp_train.to_string())\n",
    "print(\"‚Ä¢ Test:\\n\", disp_test.to_string())\n",
    "\n",
    "# Alerta simples de desbalanceamento (macro-level)\n",
    "imbalance_threshold = 0.80  # ajuste se quiser\n",
    "top_ratio = disp_overall[\"pct\"].max() / 100.0\n",
    "if top_ratio >= imbalance_threshold:\n",
    "    print(f\"[AVISO] Target potencialmente desbalanceada (classe majorit√°ria ~{top_ratio:.1%}). \"\n",
    "          \"Considere m√©tricas robustas (F1, ROC-AUC), valida√ß√£o estratificada e/ou t√©cnicas de balanceamento.\")\n",
    "\n",
    "# Sinaliza√ß√£o de categorias raras (pode explodir OHE)\n",
    "rare_threshold = 5  # frequ√™ncia m√≠nima\n",
    "rare_report = {}\n",
    "for c in cat_cols:\n",
    "    cnt = X_train[c].value_counts(dropna=False)\n",
    "    rare = cnt[cnt < rare_threshold]\n",
    "    if len(rare) > 0:\n",
    "        rare_report[c] = len(rare)\n",
    "if rare_report:\n",
    "    top_rare_cols = sorted(rare_report.items(), key=lambda kv: kv[1], reverse=True)[:10]\n",
    "    print(f\"[INFO] Colunas categ√≥ricas com categorias raras (<{rare_threshold} amostras) no train:\")\n",
    "    for col, n_rare in top_rare_cols:\n",
    "        print(f\" - {col}: {n_rare} categorias raras\")\n",
    "else:\n",
    "    print(\"[INFO] Nenhuma categoria rara detectada no train com o limite atual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4b676b-d74f-47db-b953-8d9002db45fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>PaymentMethod_len</th>\n",
       "      <th>PaymentMethod_word_count</th>\n",
       "      <th>PaymentMethod_has_error</th>\n",
       "      <th>PaymentMethod_has_cancel</th>\n",
       "      <th>PaymentMethod_has_premium</th>\n",
       "      <th>Churn_len</th>\n",
       "      <th>Churn_word_count</th>\n",
       "      <th>Churn_has_error</th>\n",
       "      <th>Churn_has_cancel</th>\n",
       "      <th>Churn_has_premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no phone service</td>\n",
       "      <td>dsl</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>34</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>dsl</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>dsl</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>45</td>\n",
       "      <td>no</td>\n",
       "      <td>no phone service</td>\n",
       "      <td>dsl</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>fiber optic</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  female              0     yes         no       1           no   \n",
       "1  5575-GNVDE    male              0      no         no      34          yes   \n",
       "2  3668-QPYBK    male              0      no         no       2          yes   \n",
       "3  7795-CFOCW    male              0      no         no      45           no   \n",
       "4  9237-HQITU  female              0      no         no       2          yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity  ... PaymentMethod_len  \\\n",
       "0  no phone service             dsl             no  ...                16   \n",
       "1                no             dsl            yes  ...                12   \n",
       "2                no             dsl            yes  ...                12   \n",
       "3  no phone service             dsl            yes  ...                25   \n",
       "4                no     fiber optic             no  ...                16   \n",
       "\n",
       "  PaymentMethod_word_count PaymentMethod_has_error PaymentMethod_has_cancel  \\\n",
       "0                        2                   False                    False   \n",
       "1                        2                   False                    False   \n",
       "2                        2                   False                    False   \n",
       "3                        3                   False                    False   \n",
       "4                        2                   False                    False   \n",
       "\n",
       "  PaymentMethod_has_premium Churn_len Churn_word_count Churn_has_error  \\\n",
       "0                     False         2                1           False   \n",
       "1                     False         2                1           False   \n",
       "2                     False         3                1           False   \n",
       "3                     False         2                1           False   \n",
       "4                     False         3                1           False   \n",
       "\n",
       "   Churn_has_cancel  Churn_has_premium  \n",
       "0             False              False  \n",
       "1             False              False  \n",
       "2             False              False  \n",
       "3             False              False  \n",
       "4             False              False  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03672d93-02fe-410a-b0d4-926559fdca98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ae9ce55",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Pr√©-processamento ‚Äî One-Hot denso + escala opcional\n",
    "\n",
    "Nesta etapa, preparamos `X` para treinamento aplicando **imputa√ß√£o**, **codifica√ß√£o categ√≥rica** e **padroniza√ß√£o opcional**.\n",
    "Mantemos o bloco **vis√≠vel no N2** por transpar√™ncia pedag√≥gica: quem l√™ o notebook consegue enxergar claramente *como* os dados chegam ao modelo.\n",
    "\n",
    "## O que acontece aqui\n",
    "\n",
    "1. **Detec√ß√£o dos conjuntos de colunas**\n",
    "   - Usamos `num_cols` e `cat_cols` (derivados de `summarize_columns(X)`) para endere√ßar o tratamento adequado a cada tipo.\n",
    "\n",
    "2. **Pipeline num√©rico**\n",
    "   - `SimpleImputer(strategy=\"mean\")` para preencher valores ausentes.\n",
    "   - `StandardScaler()` **opcional** (controlado por `cfg[\"scale_numeric\"]`), deixando as features num√©ricas com m√©dia 0 e desvio 1.\n",
    "   - Este bloco √© montado dinamicamente: se `scale_numeric=False`, o scaler √© omitido.\n",
    "\n",
    "3. **Pipeline categ√≥rico**\n",
    "   - `SimpleImputer(strategy=\"most_frequent\")` para preencher categorias ausentes.\n",
    "   - `OneHotEncoder(handle_unknown=\"ignore\", output denso)`. O c√≥digo √© **compat√≠vel** com vers√µes antigas/novas do scikit-learn:\n",
    "     - usa `sparse_output=False` quando dispon√≠vel;\n",
    "     - faz *fallback* para `sparse=False` em vers√µes anteriores.\n",
    "\n",
    "4. **ColumnTransformer**\n",
    "   - Une os dois pipelines e aplica a transforma√ß√£o **apenas √†s colunas selecionadas**, descartando o restante (`remainder=\"drop\"`).\n",
    "   - Mantemos `verbose_feature_names_out=False` para preservar nomes leg√≠veis nas colunas expandidas do One-Hot.\n",
    "\n",
    "5. **Ajuste e transforma√ß√£o**\n",
    "   - `fit` no `X_train` (usando `y_train` quando necess√°rio) e `transform` no `X_train`/`X_test`.\n",
    "   - Garantimos sa√≠da **densa** (arrays NumPy), apropriada para modelos que n√£o aceitam matrizes esparsas.\n",
    "\n",
    "6. **Diagn√≥sticos r√°pidos**\n",
    "   - Imprimimos:\n",
    "     - `scale_numeric`, tamanho de `num_cols`/`cat_cols`,\n",
    "     - quantidade total de **features transformadas**,\n",
    "     - *preview* dos **primeiros nomes** de features,\n",
    "     - `shapes` de `X_train_t`/`X_test_t`,\n",
    "     - **mem√≥ria estimada** (MB) dos arrays transformados.\n",
    "\n",
    "## Observa√ß√µes importantes\n",
    "\n",
    "- **Identificadores (IDs)** como `customerID` n√£o devem compor `cat_cols` (explodem o One-Hot e n√£o agregam sinal preditivo).  \n",
    "  Remova-os de `X` *antes* do preprocess ou exclua-os da lista de categ√≥ricas.\n",
    "- Para colunas categ√≥ricas com **alta cardinalidade**, considere alternativas como **hashing trick**, **target encoding** ou **CatBoost encoding** em vers√µes futuras do template.\n",
    "- Se a mem√≥ria ficar alta, avalie:\n",
    "  - reduzir cardinalidade (agrupar categorias raras),\n",
    "  - trocar o encoder,\n",
    "  - ou trabalhar com sa√≠da **esparsa** (e modelos que aceitem esparsidade).\n",
    "\n",
    "## Pr√≥ximo passo\n",
    "\n",
    "- Encaixar o `preprocess` nos **Pipelines** dos modelos (Dummy, LogisticRegression, KNN, RandomForest, etc.) e seguir para o **seletor de modelos e hiperpar√¢metros (UI)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0cf3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] scale_numeric=True | num_cols=62 | cat_cols=65\n",
      "[INFO] Features transformadas: 5787\n",
      "[INFO] Preview das primeiras 20 features:\n",
      " - SeniorCitizen\n",
      " - tenure\n",
      " - MonthlyCharges\n",
      " - TotalCharges\n",
      " - SeniorCitizen_was_missing\n",
      " - tenure_was_missing\n",
      " - MonthlyCharges_was_missing\n",
      " - TotalCharges_was_missing\n",
      " - customerID_was_missing\n",
      " - gender_was_missing\n",
      " - Partner_was_missing\n",
      " - Dependents_was_missing\n",
      " - PhoneService_was_missing\n",
      " - MultipleLines_was_missing\n",
      " - InternetService_was_missing\n",
      " - OnlineSecurity_was_missing\n",
      " - OnlineBackup_was_missing\n",
      " - DeviceProtection_was_missing\n",
      " - TechSupport_was_missing\n",
      " - StreamingTV_was_missing\n",
      "[INFO] Shapes transformados -> X_train_t: (5634, 5787) | X_test_t: (1409, 5787)\n",
      "[INFO] Mem√≥ria estimada -> train=248.75 MB | test=62.21 MB\n"
     ]
    }
   ],
   "source": [
    "# === Pr√©-processamento (One-Hot denso + escala opcional) ===\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fun√ß√£o compat√≠vel com vers√µes antigas/novas do scikit-learn\n",
    "def build_preprocess(numeric_cols, categorical_cols, scale_numeric=True):\n",
    "    \"\"\"\n",
    "    Cria um ColumnTransformer com:\n",
    "      - Num√©ricas: imputa√ß√£o m√©dia + (opcional) StandardScaler\n",
    "      - Categ√≥ricas: imputa√ß√£o mais frequente + OneHotEncoder denso (compat√≠vel com vers√µes)\n",
    "    \"\"\"\n",
    "    # 1) OneHotEncoder compat√≠vel com sklearn >= 1.4 (sparse_output) e vers√µes anteriores (sparse)\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "    # 2) Pipeline num√©rico\n",
    "    num_steps = [(\"imputer\", SimpleImputer(strategy=\"mean\"))]\n",
    "    if scale_numeric and len(numeric_cols) > 0:\n",
    "        num_steps.append((\"scaler\", StandardScaler()))\n",
    "    numeric_transformer = Pipeline(steps=num_steps)\n",
    "\n",
    "    # 3) Pipeline categ√≥rico\n",
    "    cat_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe),\n",
    "    ])\n",
    "\n",
    "    # 4) ColumnTransformer\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_cols if len(numeric_cols) > 0 else []),\n",
    "            (\"cat\", cat_transformer, categorical_cols if len(categorical_cols) > 0 else []),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    return ct\n",
    "\n",
    "\n",
    "# ---- Constru√ß√£o do pr√©-processador conforme config\n",
    "scale_numeric = bool(cfg.get(\"scale_numeric\", True))\n",
    "preprocess = build_preprocess(num_cols, cat_cols, scale_numeric=scale_numeric)\n",
    "print(f\"[INFO] scale_numeric={scale_numeric} | num_cols={len(num_cols)} | cat_cols={len(cat_cols)}\")\n",
    "\n",
    "# ---- Ajuste no treino\n",
    "preprocess.fit(X_train, y_train)\n",
    "\n",
    "# ---- Tentativa de inspecionar nomes de features geradas\n",
    "feat_names = None\n",
    "try:\n",
    "    feat_names = preprocess.get_feature_names_out()\n",
    "    print(f\"[INFO] Features transformadas: {len(feat_names)}\")\n",
    "    print(\"[INFO] Preview das primeiras 20 features:\")\n",
    "    for n in feat_names[:20]:\n",
    "        print(\" -\", n)\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] N√£o foi poss√≠vel obter nomes de features: {e}\")\n",
    "\n",
    "# ---- Transforma√ß√£o de treino e teste (densa)\n",
    "X_train_t = preprocess.transform(X_train)\n",
    "X_test_t  = preprocess.transform(X_test)\n",
    "\n",
    "# Garante array denso (caso algum backend retorne esparso)\n",
    "if hasattr(X_train_t, \"toarray\"):\n",
    "    X_train_t = X_train_t.toarray()\n",
    "if hasattr(X_test_t, \"toarray\"):\n",
    "    X_test_t = X_test_t.toarray()\n",
    "\n",
    "print(f\"[INFO] Shapes transformados -> X_train_t: {X_train_t.shape} | X_test_t: {X_test_t.shape}\")\n",
    "\n",
    "# ---- (Opcional) Relato r√°pido de mem√≥ria\n",
    "def _mb(nbytes): \n",
    "    return f\"{(nbytes or 0) / (1024**2):.2f} MB\"\n",
    "\n",
    "try:\n",
    "    mem_train = X_train_t.nbytes if isinstance(X_train_t, np.ndarray) else None\n",
    "    mem_test  = X_test_t.nbytes  if isinstance(X_test_t,  np.ndarray) else None\n",
    "    print(f\"[INFO] Mem√≥ria estimada -> train={_mb(mem_train)} | test={_mb(mem_test)}\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---- (Opcional) DataFrame de features apenas para DEBUG/inspe√ß√£o (cuidado com mem√≥ria!)\n",
    "# Ative s√≥ quando necess√°rio; por padr√£o, mantemos como arrays numpy eficientes.\n",
    "# if feat_names is not None:\n",
    "#     X_train_df = pd.DataFrame(X_train_t, columns=feat_names, index=X_train.index)\n",
    "#     X_test_df  = pd.DataFrame(X_test_t,  columns=feat_names, index=X_test.index)\n",
    "#     display(X_train_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3429408",
   "metadata": {},
   "source": [
    "## 4) Seletor de modelos e hiperpar√¢metros (UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0378c7b8-ff61-448c-bb75-0bf3fd081718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "      :root{\n",
       "        --lumen-bg: radial-gradient(1200px 600px at 20% -10%, rgba(0,255,209,0.20) 0%, transparent 50%),\n",
       "                    radial-gradient(900px 600px at 110% 110%, rgba(98,0,255,0.18) 0%, transparent 40%),\n",
       "                    linear-gradient(135deg, #0a0f16 0%, #0b0e14 100%);\n",
       "        --lumen-panel: rgba(15, 22, 33, 0.55);\n",
       "        --lumen-glass: rgba(11, 18, 29, 0.5);\n",
       "        --lumen-border: rgba(0, 255, 209, 0.35);\n",
       "        --lumen-glow: 0 0 0.5rem rgba(0,255,209,0.35), inset 0 0 0.5rem rgba(0,255,209,0.1);\n",
       "        --lumen-warn: #ffb86b;\n",
       "        --lumen-accent: #00ffd1;\n",
       "        --lumen-accent-2: #9a7dff;\n",
       "        --lumen-muted: #9aa4ad;\n",
       "        --lumen-text: #e8f4ff;\n",
       "      }\n",
       "      .lumen-console {\n",
       "        background: var(--lumen-bg);\n",
       "        border-radius: 14px;\n",
       "        padding: 14px 16px;\n",
       "        border: 1px solid var(--lumen-border);\n",
       "        box-shadow: var(--lumen-glow);\n",
       "        backdrop-filter: blur(10px) saturate(110%);\n",
       "        color: var(--lumen-text);\n",
       "        font-family: ui-sans-serif, system-ui, Segoe UI, Roboto, Ubuntu, Cantarell, \"Helvetica Neue\", Arial;\n",
       "      }\n",
       "      .lumen-header { display:flex; align-items:center; gap:12px; margin-bottom:10px; }\n",
       "      .lumen-title { font-weight:700; letter-spacing:.3px; font-size:16px; color:var(--lumen-accent);\n",
       "        text-shadow: 0 0 10px rgba(0,255,209,0.4); }\n",
       "      .lumen-chip { padding:2px 8px; border:1px solid var(--lumen-border); border-radius:999px;\n",
       "        font-size:11px; color:var(--lumen-accent-2); background: rgba(154,125,255,0.07);}\n",
       "      .lumen-row { display:flex; gap:10px; align-items:center; flex-wrap:wrap; }\n",
       "      .lumen-controls { display:flex; align-items:center; gap:8px; justify-content:space-between; margin-top:8px;}\n",
       "      .lumen-warning { color: var(--lumen-warn); font-size: 12px; margin-left: 6px; opacity: .9;}\n",
       "      .lumen-note { color: var(--lumen-muted); font-size: 12px; margin-left: 4px; }\n",
       "      .widget-tab { border:1px solid var(--lumen-border)!important; border-radius:10px; overflow:hidden;\n",
       "        box-shadow: inset 0 0 10px rgba(0,255,209,0.08); background: var(--lumen-glass)!important; }\n",
       "      .widget-tab>div:nth-child(1) {\n",
       "        background: linear-gradient(90deg, rgba(0,255,209,0.06), rgba(154,125,255,0.05)) !important;\n",
       "        border-bottom: 1px solid rgba(0,255,209,0.25) !important; }\n",
       "      .widget-tab .p-TabBar-tab { color:var(--lumen-text)!important; text-shadow:0 0 8px rgba(0,255,209,0.25);\n",
       "        border-right: 1px solid rgba(0,255,209,0.10); background:transparent!important; }\n",
       "      .widget-tab .p-TabBar-tab.p-mod-current {\n",
       "        background: linear-gradient(180deg, rgba(0,255,209,0.1), rgba(0,0,0,0)) !important;\n",
       "        box-shadow: inset 0 -3px 0 var(--lumen-accent); }\n",
       "      .lumen-card { margin:8px; padding:8px 10px; border:1px dashed rgba(0,255,209,0.25);\n",
       "        border-radius:10px; background: rgba(0, 12, 20, 0.35); }\n",
       "      .lumen-label { min-width:160px; color:var(--lumen-muted); font-size:12px; }\n",
       "      .lumen-play .widget-button { background: linear-gradient(135deg, rgba(0,255,209,0.25), rgba(154,125,255,0.25));\n",
       "        color: var(--lumen-text); border:1px solid var(--lumen-border); box-shadow: var(--lumen-glow); }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509293fbd47048898a9ec2f5e3df123b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(VBox(children=(HBox(children=(HTML(value=\"<div class='lumen-title'>Seletor de modelos ¬∑ Hyperdri‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class='lumen-console' style='margin-top:8px;'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === UI: Seletor de modelos + Hyperdrive (encapsulado no utils_data) ===\n",
    "from utils.utils_data import n2_build_models_ui\n",
    "\n",
    "n2_build_models_ui(\n",
    "    preprocess=preprocess,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    models_dir=models_dir,\n",
    "    reports_dir=reports_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066d6341",
   "metadata": {},
   "source": [
    "## 5) Treino, avalia√ß√£o e export de artefatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_params_from_tab():\n",
    "    return {name: {k: w.value for k, w in spec[\"params\"].items()} for name, spec in MODEL_REGISTRY.items()}\n",
    "\n",
    "def compute_and_plot(pipe, name, X_test, y_test):\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    metrics = compute_metrics(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot(ax=ax, colorbar=False)\n",
    "    ax.set_title(f\"Matriz de confus√£o ‚Äî {name}\")\n",
    "    plt.show()\n",
    "    try_plot_roc(pipe, X_test, y_test)\n",
    "    print(f\"[OK] {name}: accuracy={metrics['accuracy']:.4f} | f1={metrics['f1']:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "def train_and_eval(models_selected, params_by_model):\n",
    "    results = {}\n",
    "    for name, selected in models_selected.items():\n",
    "        if not selected: \n",
    "            continue\n",
    "        ModelClass = MODEL_REGISTRY[name][\"class\"]\n",
    "        params = params_by_model.get(name, {})\n",
    "        pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"clf\", ModelClass(**params))])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        metrics = compute_and_plot(pipe, name, X_test, y_test)\n",
    "        results[name] = {\"pipeline\": pipe, \"metrics\": metrics, \"params\": params}\n",
    "    return results\n",
    "\n",
    "@out.capture()\n",
    "def on_train_clicked(_):\n",
    "    clear_output(wait=True)\n",
    "    display(W.HTML(\"<h4>Treinando...</h4>\"))\n",
    "    selected = {name: chk.value for name, chk in model_checks.items()}\n",
    "    params = collect_params_from_tab()\n",
    "    results = train_and_eval(selected, params)\n",
    "\n",
    "    if results:\n",
    "        df_rank = pd.DataFrame([{\"model\": k, **v[\"metrics\"], **{\"params\": v[\"params\"]}} for k, v in results.items()])\\\n",
    "                    .sort_values(by=[\"f1\", \"accuracy\"], ascending=False)\n",
    "        display(W.HTML(\"<h4>Ranking (F1, depois Accuracy)</h4>\")); display(df_rank)\n",
    "        if cb_persist.value:\n",
    "            for name, rec in results.items():\n",
    "                persist_artifacts(name, rec[\"pipeline\"], rec[\"metrics\"], rec[\"params\"], models_dir, reports_dir)\n",
    "    else:\n",
    "        print(\"[AVISO] Nenhum modelo selecionado.\")\n",
    "\n",
    "btn_train.on_click(on_train_clicked)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
